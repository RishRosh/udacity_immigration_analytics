{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id, lit, substring\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# logging setup\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dl.cfg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configuration\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID']=config['S3']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['S3']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "    config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "    enableHiveSupport().getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data = \"\"\n",
    "sas_input_dir  = '../../data/18-83510-I94-Data-2016/'\n",
    "sas_input_file = 'i94_apr16_sub.sas7bdat'\n",
    "output_data = \"sas_data2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark Version :2.4.3\n",
      "Apache Spark Version :2.4.3\n"
     ]
    }
   ],
   "source": [
    "print('Apache Spark Version :'+spark.version)\n",
    "print('Apache Spark Version :'+spark.sparkContext.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_demo(spark, input_data):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # start logging message\n",
    "    logging.info(\"Started loading demographics data\")\n",
    "    \n",
    "    # load the file\n",
    "    f_demo = os.path.join(input_data + 'us-cities-demographics.csv')\n",
    "    df = spark.read.format('csv').options(header=True, delimiter=';').load(f_demo)\n",
    "    \n",
    "    # Get only selected columns and de-dupe them\n",
    "    df_demo = df.select(\"City\", \"Median Age\", \"Male Population\", \"Female Population\", \"Total Population\", \\\n",
    "                        \"Number of Veterans\", \"Foreign-born\", \"Average Household Size\", \"State Code\")\\\n",
    "                .drop_duplicates()\n",
    "    \n",
    "    # end logging message and return the dataframe\n",
    "    logging.info(\"Completed loading US Cities data\")\n",
    "    return df_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Started loading demographics data\n",
      "INFO:root:Completed loading US Cities data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+\n",
      "|            City|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|\n",
      "+----------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+\n",
      "|      Providence|      29.9|          89090|            90114|          179204|              4933|       53532|                  2.72|        RI|\n",
      "|      Lewisville|      31.6|          52776|            52032|          104808|              4211|       24865|                  2.78|        TX|\n",
      "|          Layton|      29.5|          37748|            36394|           74142|              3811|        4268|                  3.24|        UT|\n",
      "|        San Juan|      41.4|         155408|           186829|          342237|              null|        null|                  null|        PR|\n",
      "|          Weston|      38.6|          32956|            36991|           69947|              1507|       30876|                  3.34|        FL|\n",
      "|    Fayetteville|      30.7|         101051|           100914|          201965|             28089|       12863|                   2.5|        NC|\n",
      "|          Topeka|      37.1|          60879|            66378|          127257|              9608|        6711|                  2.32|        KS|\n",
      "|         Hialeah|      43.0|         111530|           125552|          237082|              1844|      170148|                  3.31|        FL|\n",
      "|      Fort Myers|      37.3|          36850|            37165|           74015|              4312|       15365|                  2.45|        FL|\n",
      "|        Woodbury|      37.7|          31982|            35868|           67850|              2732|        9477|                  2.72|        MN|\n",
      "|   Mission Viejo|      44.9|          48849|            48314|           97163|              4713|       17308|                  2.85|        CA|\n",
      "|Huntington Beach|      41.6|          99967|           101960|          201927|             10454|       35368|                  2.67|        CA|\n",
      "|      Union City|      35.4|          35376|            33773|           69149|               705|       40553|                  2.85|        NJ|\n",
      "|       Iowa City|      25.5|          37089|            37138|           74227|              2047|        9202|                  2.36|        IA|\n",
      "|    Baldwin Park|      35.8|          38747|            38309|           77056|               780|       34322|                  4.13|        CA|\n",
      "|    Flower Mound|      40.2|          35200|            35824|           71024|              4217|        6860|                  3.01|        TX|\n",
      "|         Deltona|      39.9|          44853|            43621|           88474|              5664|        7098|                  2.89|        FL|\n",
      "|          Warren|      43.2|          64063|            71293|          135356|              7806|       18822|                  2.45|        MI|\n",
      "|      Somerville|      31.0|          41028|            39306|           80334|              2103|       22292|                  2.43|        MA|\n",
      "|        Meridian|      38.0|          43353|            47400|           90753|              5640|        8110|                  2.54|        ID|\n",
      "+----------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo = load_demo(spark, input_data)\n",
    "df_demo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_us_cities(spark, input_data):\n",
    "    # start logging message\n",
    "    logging.info(\"Started loading US Cities data\")\n",
    "    \n",
    "    # load file\n",
    "    f_uscit = os.path.join(input_data + 'uscities.csv')\n",
    "    df = spark.read.format('csv').options(header=True).load(f_uscit)\n",
    "    \n",
    "    # select only the relevant columns\n",
    "    df_cit = df.select(\"id\", \"city\", \"state_id\", \"state_name\", \"lat\",\"lng\")\\\n",
    "                .drop_duplicates()\n",
    "    \n",
    "    # end logging and return dataframe\n",
    "    logging.info(\"Completed loading US Cities data\")\n",
    "    return df_cit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Started loading US Cities data\n",
      "INFO:root:Completed loading US Cities data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+--------+--------------+-------+---------+\n",
      "|        id|              city|state_id|    state_name|    lat|      lng|\n",
      "+----------+------------------+--------+--------------+-------+---------+\n",
      "|1840020578|  Huntington Beach|      CA|    California|33.6960|-118.0018|\n",
      "|1840008192|            Joliet|      IL|      Illinois|41.5188| -88.1499|\n",
      "|1840019612|          Beaumont|      TX|         Texas|30.0849| -94.1451|\n",
      "|1840011319|        Schaumburg|      IL|      Illinois|42.0308| -88.0838|\n",
      "|1840015633|          Marietta|      GA|       Georgia|33.9533| -84.5422|\n",
      "|1840020295|            Newark|      CA|    California|37.5201|-122.0307|\n",
      "|1840021532|          Danville|      CA|    California|37.8121|-121.9698|\n",
      "|1840020173|    Pleasant Grove|      UT|          Utah|40.3716|-111.7412|\n",
      "|1840024507|      South Riding|      VA|      Virginia|38.9120| -77.5132|\n",
      "|1840021561|        Washington|      UT|          Utah|37.1303|-113.4878|\n",
      "|1840008132|        Oak Forest|      IL|      Illinois|41.6054| -87.7527|\n",
      "|1840006040|            Tysons|      VA|      Virginia|38.9215| -77.2273|\n",
      "|1840003433|          Sandusky|      OH|          Ohio|41.4468| -82.7024|\n",
      "|1840021219|       Wilsonville|      OR|        Oregon|45.3109|-122.7702|\n",
      "|1840033373|        Parsippany|      NJ|    New Jersey|40.8645| -74.4135|\n",
      "|1840014718|North Myrtle Beach|      SC|South Carolina|33.8232| -78.7082|\n",
      "|1840000549|         Willowick|      OH|          Ohio|41.6342| -81.4678|\n",
      "|1840017779|        Los Alamos|      NM|    New Mexico|35.8927|-106.2862|\n",
      "|1840003075|             Ionia|      MI|      Michigan|42.9773| -85.0727|\n",
      "|1840010463|          Yorktown|      IN|       Indiana|40.1830| -85.5123|\n",
      "+----------+------------------+--------+--------------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cit = load_us_cities(spark, input_data)\n",
    "df_cit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_airport(spark, input_data):\n",
    "    # start logging message\n",
    "    logging.info(\"Started loading airport codes data\")\n",
    "\n",
    "    # load file\n",
    "    f_air = os.path.join(input_data + 'airport-codes_csv.csv')\n",
    "    df = spark.read.format('csv').options(header=True).load(f_air)\n",
    "\n",
    "    # transform and de-dupe data\n",
    "    df_air = df.where(df['iso_country']==\"US\")\\\n",
    "            .select(\"municipality\", \"iata_code\", \"local_code\", \"iso_region\") \\\n",
    "            .withColumn(\"country\", substring(\"iso_region\",1,2))\\\n",
    "            .withColumn(\"state_code\", substring(\"iso_region\",4,2)) \\\n",
    "            .na.drop(subset=[\"iata_code\",\"local_code\"]) \\\n",
    "            .drop_duplicates()\n",
    "    df_air = df_air.select(\"municipality\", \"iata_code\", \"local_code\", \"state_code\")\n",
    "\n",
    "    # end logging message and return data frame\n",
    "    logging.info(\"Completed loading airport codes data\")\n",
    "    return df_air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport = load_airport(spark, input_data)\n",
    "df_airport.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_city_demo_airport(df_airport, df_cit, df_demo):\n",
    "\n",
    "    # Join city and airport data\n",
    "    logging.info(\"Joining airport codes and city data\")\n",
    "    df_ap_cit = df_airport.join(df_cit, (df_airport.municipality==df_cit.city) \\\n",
    "                           & (df_airport.state_code==df_cit.state_id))\\\n",
    "            .select(\"id\", \"city\", \"iata_code\", \"state_id\", \"state_name\", \"lat\", \"lng\" )\n",
    "\n",
    "    # Join demographics data to it\n",
    "    logging.info(\"Joining demographics data\")\n",
    "    df_ap_cit_demo = df_ap_cit.join(df_demo, (df_ap_cit.city==df_demo.City) \\\n",
    "                        & (df_ap_cit.state_id==df_demo[\"State Code\"]))\\\n",
    "                    .select(\"id\", df_ap_cit[\"city\"], \"iata_code\", \"state_id\", \"state_name\", \"lat\", \"lng\"\\\n",
    "                        ,col(\"Median Age\").alias(\"median_age\")\\\n",
    "                        ,col(\"Male Population\").alias(\"male_population\")\\\n",
    "                        ,col(\"Female Population\").alias(\"female_population\")\\\n",
    "                        ,col(\"Total Population\").alias(\"total_population\")\\\n",
    "                        ,col(\"Number of Veterans\").alias(\"no_of_veterans\")\\\n",
    "                        ,col(\"Foreign-born\").alias(\"foreign_born\")\\\n",
    "                        ,col(\"Average Household Size\").alias(\"avg_household_size\"))\n",
    "    \n",
    "    # write out the data\n",
    "    df_ap_cit_demo.write.parquet(os.path.join(output_data, \"cit_demo_air/\"), mode=\"overwrite\")\n",
    "    logging.info(\"Processed city, demographics and airport data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "process_city_demo_airport(df_airport, df_cit, df_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+--------+--------------------+-------+---------+----------+---------------+-----------------+----------------+--------------+------------+------------------+\n",
      "|        id|        city|iata_code|state_id|          state_name|    lat|      lng|median_age|male_population|female_population|total_population|no_of_veterans|foreign_born|avg_household_size|\n",
      "+----------+------------+---------+--------+--------------------+-------+---------+----------+---------------+-----------------+----------------+--------------+------------+------------------+\n",
      "|1840000494|     Chicago|      ORD|      IL|            Illinois|41.8375| -87.6866|      34.2|        1320015|          1400541|         2720556|         72042|      573463|              2.53|\n",
      "|1840000494|     Chicago|      MDW|      IL|            Illinois|41.8375| -87.6866|      34.2|        1320015|          1400541|         2720556|         72042|      573463|              2.53|\n",
      "|1840004773|    Hartford|      HFD|      CT|         Connecticut|41.7661| -72.6834|      31.0|          58042|            65972|          124014|          3130|       28467|              2.59|\n",
      "|1840004773|    Hartford|      BDL|      CT|         Connecticut|41.7661| -72.6834|      31.0|          58042|            65972|          124014|          3130|       28467|              2.59|\n",
      "|1840013490|Fayetteville|      POB|      NC|      North Carolina|35.0850| -78.9772|      30.7|         101051|           100914|          201965|         28089|       12863|               2.5|\n",
      "|1840013490|Fayetteville|      FAY|      NC|      North Carolina|35.0850| -78.9772|      30.7|         101051|           100914|          201965|         28089|       12863|               2.5|\n",
      "|1840015913| Tallahassee|      TLH|      FL|             Florida|30.4551| -84.2527|      26.2|          89390|           100504|          189894|          9575|       16720|              2.38|\n",
      "|1840008987|   Rochester|      RST|      MN|           Minnesota|44.0154| -92.4780|      35.0|          54934|            57282|          112216|          6888|       17763|              2.55|\n",
      "|1840015336|  Springdale|      SPZ|      AR|            Arkansas|36.1901| -94.1574|      31.8|          36840|            43614|           80454|          3397|       19969|              3.04|\n",
      "|1840006060|  Washington|      IAD|      DC|District of Columbia|38.9047| -77.0163|      33.8|         319705|           352523|          672228|         25963|       95117|              2.24|\n",
      "|1840006060|  Washington|      JPN|      DC|District of Columbia|38.9047| -77.0163|      33.8|         319705|           352523|          672228|         25963|       95117|              2.24|\n",
      "|1840006060|  Washington|      BOF|      DC|District of Columbia|38.9047| -77.0163|      33.8|         319705|           352523|          672228|         25963|       95117|              2.24|\n",
      "|1840006060|  Washington|      DCA|      DC|District of Columbia|38.9047| -77.0163|      33.8|         319705|           352523|          672228|         25963|       95117|              2.24|\n",
      "|1840015457|     Memphis|      MEM|      TN|           Tennessee|35.1087| -89.9663|      34.1|         312237|           343523|          655760|         31189|       43318|              2.55|\n",
      "|1840021639|     Visalia|      VIS|      CA|          California|36.3276|-119.3269|      32.6|          63695|            66399|          130094|          5547|       20779|              3.11|\n",
      "|1840002983|  Manchester|      MHT|      NH|       New Hampshire|42.9848| -71.4447|      37.3|          54845|            55378|          110223|          5473|       14506|               2.4|\n",
      "|1840000408|    Lawrence|      LWM|      MA|       Massachusetts|42.7002| -71.1626|      30.8|          37464|            42776|           80240|          1190|       29170|              3.17|\n",
      "|1840020491| Los Angeles|      WHP|      CA|          California|34.1141|-118.4068|      35.0|        1958998|          2012898|         3971896|         85417|     1485425|              2.86|\n",
      "|1840020491| Los Angeles|      LAX|      CA|          California|34.1141|-118.4068|      35.0|        1958998|          2012898|         3971896|         85417|     1485425|              2.86|\n",
      "|1840020491| Los Angeles|      CCD|      CA|          California|34.1141|-118.4068|      35.0|        1958998|          2012898|         3971896|         85417|     1485425|              2.86|\n",
      "+----------+------------+---------+--------+--------------------+-------+---------+----------+---------------+-----------------+----------------+--------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cit_dem_air=spark.read.parquet(\"sas_data2/cit_demo_air\")\n",
    "df_cit_dem_air.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-----+\n",
      "|        id|        city|state_code|count|\n",
      "+----------+------------+----------+-----+\n",
      "|1840020925|     Houston|        TX|    9|\n",
      "|1840034016|    New York|        NY|    6|\n",
      "|1840015031|Jacksonville|        FL|    5|\n",
      "|1840015149|       Miami|        FL|    5|\n",
      "|1840021990|   San Diego|        CA|    5|\n",
      "|1840006060|  Washington|        DC|    4|\n",
      "|1840020696|  Fort Worth|        TX|    4|\n",
      "|1840022220| San Antonio|        TX|    4|\n",
      "|1840034249|      Dayton|        OH|    4|\n",
      "|1840020364|   Las Vegas|        NV|    4|\n",
      "|1840021491|  Sacramento|        CA|    4|\n",
      "|1840001686|     Wichita|        KS|    4|\n",
      "|1840015982|       Tampa|        FL|    4|\n",
      "|1840015099|     Orlando|        FL|    4|\n",
      "|1840019941|    Portland|        OR|    3|\n",
      "|1840003971|     Detroit|        MI|    3|\n",
      "|1840019440|      Dallas|        TX|    3|\n",
      "|1840021117|     Seattle|        WA|    3|\n",
      "|1840000596|   Cleveland|        OH|    3|\n",
      "|1840000673|Philadelphia|        PA|    3|\n",
      "+----------+------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test code\n",
    "df2 = df_n \\\n",
    "    .groupby(['id','city', 'state_code']) \\\n",
    "    .count() \\\n",
    "    .where('count > 1') \\\n",
    "    .sort('count', ascending=False) \n",
    "\n",
    "df2.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test code\n",
    "df2.groupby(['id']) \\\n",
    "    .count() \\\n",
    "    .where('count > 1') \\\n",
    "    .sort('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i94_apr16_sub.sas7bdat\n",
      "i94_sep16_sub.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "# test code\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = '../../data/18-83510-I94-Data-2016/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "tf = 'i94_jan16_sub.sas7bdat'\n",
    "j=0\n",
    "for i in onlyfiles:\n",
    "    j+=1 \n",
    "    if i!=tf and j<3:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test code\n",
    "sas_input_data_dir  = '../../data/18-83510-I94-Data-2016/'\n",
    "immi_data1 = os.path.join(sas_input_data_dir + 'i94_apr16_sub.sas7bdat')\n",
    "immi_data2 = os.path.join(sas_input_data_dir + 'i94_mar16_sub.sas7bdat')\n",
    "#     spark.read.format(\"com.github.saurfang.sas.spark\").load(\"/users/shobhana/sas_files\", pathGlobFilter=\"*.sas7bdat\")\n",
    "df1 = spark.read.format(\"com.github.saurfang.sas.spark\").load(immi_data1)\n",
    "df2 = spark.read.format(\"com.github.saurfang.sas.spark\").load(immi_data2)\n",
    "df3 = df1.union(df2)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_immi(spark, sas_file):\n",
    "    logging.info(\"Started loading immigration data\")\n",
    "    df = spark.read.format('com.github.saurfang.sas.spark')\\\n",
    "            .load(sas_file)\n",
    "    \n",
    "    # function to convert sas date\n",
    "    date_format = \"%Y-%m-%d\"\n",
    "    convert_sas_udf = udf(lambda x: x if x is None else (timedelta(days=x) + datetime(1960, 1, 1)).strftime(date_format))\n",
    "\n",
    "    # process immigration data\n",
    "    df_immi = df.select(\"cicid\", \"arrdate\", \"i94yr\", \"i94mon\", \"i94visa\", \"i94port\", \"airline\", \"fltno\", \"visatype\")\\\n",
    "                .na.drop(subset=[\"arrdate\", \"i94yr\", \"i94mon\", \"i94visa\", \"i94port\", \"airline\", \"fltno\", \"visatype\"]) \\\n",
    "                .withColumn(\"cicid\", col(\"cicid\").cast(\"int\")) \\\n",
    "                .withColumn(\"i94mon\", col(\"i94mon\").cast(\"int\")) \\\n",
    "                .withColumn(\"i94yr\", col(\"i94yr\").cast(\"int\")) \\\n",
    "                .withColumn(\"i94visa\", col(\"i94visa\").cast(\"int\")) \\\n",
    "                .withColumn(\"arrdate\", convert_sas_udf(\"arrdate\")) \\\n",
    "                .withColumn(\"mon\", col(\"i94mon\").cast(\"int\")) \\\n",
    "                .withColumn(\"yr\", col(\"i94yr\").cast(\"int\")) \\\n",
    "                .drop_duplicates()\n",
    "    \n",
    "    logging.info(\"Finished loading immigration data\")\n",
    "    return df_immi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Started loading immigration data\n",
      "INFO:root:Finished loading immigration data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+------+-------+-------+-------+-----+--------+---+----+\n",
      "|cicid|   arrdate|i94yr|i94mon|i94visa|i94port|airline|fltno|visatype|mon|  yr|\n",
      "+-----+----------+-----+------+-------+-------+-------+-----+--------+---+----+\n",
      "|  778|2016-12-01| 2016|    12|      2|    XXX|     TK|00009|      B2| 12|2016|\n",
      "| 1585|2016-12-01| 2016|    12|      2|    NYC|     WS| 1216|      WT| 12|2016|\n",
      "| 1630|2016-12-01| 2016|    12|      2|    AGA|     UA|00136|     GMT| 12|2016|\n",
      "| 1839|2016-12-01| 2016|    12|      2|    LOS|     LA|00600|      WT| 12|2016|\n",
      "| 2003|2016-12-01| 2016|    12|      2|    AGA|     UA|00827|     GMT| 12|2016|\n",
      "| 2166|2016-12-01| 2016|    12|      2|    AGA|     UA|00827|     GMT| 12|2016|\n",
      "| 2243|2016-12-01| 2016|    12|      2|    AGA|     UA|00827|     GMT| 12|2016|\n",
      "| 4865|2016-12-01| 2016|    12|      2|    AGA|     7C|03154|     GMT| 12|2016|\n",
      "| 5388|2016-12-01| 2016|    12|      2|    AGA|     UA|00178|     GMT| 12|2016|\n",
      "| 5586|2016-12-01| 2016|    12|      2|    AGA|     DL|00608|     GMT| 12|2016|\n",
      "| 6062|2016-12-01| 2016|    12|      2|    SAI|     ZE|00581|     GMT| 12|2016|\n",
      "| 6717|2016-12-01| 2016|    12|      2|    AGA|     JL|00941|     GMT| 12|2016|\n",
      "| 7120|2016-12-01| 2016|    12|      2|    AGA|     TW|00301|     GMT| 12|2016|\n",
      "| 7208|2016-12-01| 2016|    12|      2|    AGA|     UA|00166|     GMT| 12|2016|\n",
      "| 7641|2016-12-01| 2016|    12|      2|    AGA|     KE|00111|     GMT| 12|2016|\n",
      "| 7826|2016-12-01| 2016|    12|      2|    SAI|     7C|03402|     GMT| 12|2016|\n",
      "| 7896|2016-12-01| 2016|    12|      2|    AGA|     7C|03106|     GMT| 12|2016|\n",
      "| 7994|2016-12-01| 2016|    12|      2|    SAI|     7C|03402|     GMT| 12|2016|\n",
      "| 8299|2016-12-01| 2016|    12|      2|    SAI|     7C|03404|     GMT| 12|2016|\n",
      "| 8301|2016-12-01| 2016|    12|      2|    SAI|     7C|03404|     GMT| 12|2016|\n",
      "+-----+----------+-----+------+-------+-------+-------+-----+--------+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test code\n",
    "sas_input_file = 'i94_dec16_sub.sas7bdat'\n",
    "sas_file = join(sas_input_dir, sas_input_file)\n",
    "sas_file\n",
    "df_immi = load_immi(spark, sas_file)\n",
    "df_immi.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Started processing immigration file i94_apr16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_sep16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_nov16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_mar16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_jun16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_aug16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_may16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_jan16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_oct16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_jul16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_feb16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_dec16_sub.sas7bdat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i94_apr16_sub.sas7bdat\n",
      "i94_sep16_sub.sas7bdat\n",
      "i94_nov16_sub.sas7bdat\n",
      "i94_mar16_sub.sas7bdat\n",
      "i94_jun16_sub.sas7bdat\n",
      "i94_aug16_sub.sas7bdat\n",
      "i94_may16_sub.sas7bdat\n",
      "i94_jan16_sub.sas7bdat\n",
      "i94_oct16_sub.sas7bdat\n",
      "i94_jul16_sub.sas7bdat\n",
      "i94_feb16_sub.sas7bdat\n",
      "i94_dec16_sub.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "onlyfiles = [f for f in listdir(sas_input_dir) if isfile(join(sas_input_dir, f))]\n",
    "df_immi = None\n",
    "for i in onlyfiles:\n",
    "    logging.info(\"Started processing immigration file {}\".format(i))\n",
    "    if df_immi is None:\n",
    "        print(i)\n",
    "#         df_nnn = load_demo(spark, input_data)\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_immi(spark, sas_input_dir, output_data): \n",
    "\n",
    "    # process all files\n",
    "    df_immi = None\n",
    "    df_t = None\n",
    "    onlyfiles = [f for f in listdir(sas_input_dir) if isfile(join(sas_input_dir, f))]\n",
    "    for i in onlyfiles:\n",
    "        logging.info(\"Started processing immigration file {}\".format(i))\n",
    "        sas_file = join(sas_input_dir, i)\n",
    "        if df_immi is None:\n",
    "            df_immi = load_immi(spark, sas_file)\n",
    "        else:\n",
    "            df_t = load_immi(spark, sas_file)\n",
    "            df_immi = df_immi.union(df_t)\n",
    "        logging.info(\"Finished processing immigration file {}\".format(i))\n",
    "    \n",
    "    logging.info(\"Completed processing immigration data\")\n",
    "    logging.info(\"Writing out immigration data\")\n",
    "    \n",
    "    # write immi data \n",
    "    df_immi.write.parquet(os.path.join(output_data, \"immigration/\"), mode=\"overwrite\", partitionBy=[\"yr\",\"mon\"])\n",
    "    logging.info(\"Writing immigration data completed\")\n",
    "    \n",
    "    \n",
    "    logging.info(\"Processing time data\")\n",
    "    # process time data\n",
    "    time_table = df_immi.withColumn(\"day\",dayofmonth(\"arrdate\"))\\\n",
    "                    .withColumn(\"week\",weekofyear(\"arrdate\"))\\\n",
    "                    .withColumn(\"month\",month(\"arrdate\"))\\\n",
    "                    .withColumn(\"year\",year(\"arrdate\"))\\\n",
    "                    .withColumn(\"weekday\",dayofweek(\"arrdate\")) \\\n",
    "                    .select(\"arrdate\", \"day\", \"week\", \"month\", \"year\", \"weekday\").drop_duplicates()\n",
    "    \n",
    "    # write time data \n",
    "    time_table.write.parquet(os.path.join(output_data, \"time/\"), mode=\"overwrite\")\n",
    "    logging.info(\"Time data processed\")\n",
    "    \n",
    "    logging.info(\"Processing flights data\")\n",
    "    \n",
    "    # process flights data\n",
    "    flights = df_immi.select(\"airline\", \"fltno\").drop_duplicates()\n",
    "                \n",
    "    # write flights data \n",
    "    flights.write.parquet(os.path.join(output_data, \"flights/\"), mode=\"overwrite\")\n",
    "    logging.info(\"Flights data processed\")\n",
    "    \n",
    "    logging.info(\"Processing visa data\")\n",
    "    \n",
    "    # process visa data\n",
    "    visas = df_immi.select(\"i94visa\", \"visatype\").drop_duplicates()\n",
    "                    \n",
    "    # write visa data \n",
    "    visas.write.parquet(os.path.join(output_data, \"visas/\"), mode=\"overwrite\")\n",
    "    logging.info(\"Visa data processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Started processing immigration file i94_apr16_sub.sas7bdat\n",
      "INFO:root:Started loading immigration data\n",
      "INFO:root:Finished loading immigration data\n",
      "INFO:root:Finished processing immigration file i94_apr16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_sep16_sub.sas7bdat\n",
      "INFO:root:Started loading immigration data\n",
      "INFO:root:Finished loading immigration data\n",
      "INFO:root:Finished processing immigration file i94_sep16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_nov16_sub.sas7bdat\n",
      "INFO:root:Started loading immigration data\n",
      "INFO:root:Finished loading immigration data\n",
      "INFO:root:Finished processing immigration file i94_nov16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_mar16_sub.sas7bdat\n",
      "INFO:root:Started loading immigration data\n",
      "INFO:root:Finished loading immigration data\n",
      "INFO:root:Finished processing immigration file i94_mar16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_jun16_sub.sas7bdat\n",
      "INFO:root:Started loading immigration data\n",
      "INFO:root:Finished loading immigration data\n",
      "INFO:root:Finished processing immigration file i94_jun16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_aug16_sub.sas7bdat\n",
      "INFO:root:Started loading immigration data\n",
      "INFO:root:Finished loading immigration data\n",
      "INFO:root:Finished processing immigration file i94_aug16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_may16_sub.sas7bdat\n",
      "INFO:root:Started loading immigration data\n",
      "INFO:root:Finished loading immigration data\n",
      "INFO:root:Finished processing immigration file i94_may16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_jan16_sub.sas7bdat\n",
      "INFO:root:Started loading immigration data\n",
      "INFO:root:Finished loading immigration data\n",
      "INFO:root:Finished processing immigration file i94_jan16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_oct16_sub.sas7bdat\n",
      "INFO:root:Started loading immigration data\n",
      "INFO:root:Finished loading immigration data\n",
      "INFO:root:Finished processing immigration file i94_oct16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_jul16_sub.sas7bdat\n",
      "INFO:root:Started loading immigration data\n",
      "INFO:root:Finished loading immigration data\n",
      "INFO:root:Finished processing immigration file i94_jul16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_feb16_sub.sas7bdat\n",
      "INFO:root:Started loading immigration data\n",
      "INFO:root:Finished loading immigration data\n",
      "INFO:root:Finished processing immigration file i94_feb16_sub.sas7bdat\n",
      "INFO:root:Started processing immigration file i94_dec16_sub.sas7bdat\n",
      "INFO:root:Started loading immigration data\n",
      "INFO:root:Finished loading immigration data\n",
      "INFO:root:Finished processing immigration file i94_dec16_sub.sas7bdat\n",
      "INFO:root:Completed processing immigration data\n",
      "INFO:root:Writing out immigration data\n",
      "INFO:root:Writing immigration data completed\n",
      "INFO:root:Processing time data\n",
      "INFO:root:Time data processed\n",
      "INFO:root:Processing flights data\n",
      "INFO:root:Flights data processed\n",
      "INFO:root:Processing visa data\n",
      "INFO:root:Visa data processed\n"
     ]
    }
   ],
   "source": [
    "process_immi(spark, sas_input_dir, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----+-----+----+-------+\n",
      "|   arrdate|day|week|month|year|weekday|\n",
      "+----------+---+----+-----+----+-------+\n",
      "|2016-09-07|  7|  36|    9|2016|      4|\n",
      "|2016-09-19| 19|  38|    9|2016|      2|\n",
      "|2016-09-21| 21|  38|    9|2016|      4|\n",
      "|2016-01-21| 21|   3|    1|2016|      5|\n",
      "|2016-02-13| 13|   6|    2|2016|      7|\n",
      "|2016-02-11| 11|   6|    2|2016|      5|\n",
      "|2016-11-12| 12|  45|   11|2016|      7|\n",
      "|2016-03-20| 20|  11|    3|2016|      1|\n",
      "|2016-05-05|  5|  18|    5|2016|      5|\n",
      "|2016-05-31| 31|  22|    5|2016|      3|\n",
      "|2016-07-21| 21|  29|    7|2016|      5|\n",
      "|2016-07-28| 28|  30|    7|2016|      5|\n",
      "|2016-04-02|  2|  13|    4|2016|      7|\n",
      "|2016-08-09|  9|  32|    8|2016|      3|\n",
      "|2016-08-16| 16|  33|    8|2016|      3|\n",
      "|2016-08-17| 17|  33|    8|2016|      4|\n",
      "|2016-01-07|  7|   1|    1|2016|      5|\n",
      "|2016-10-20| 20|  42|   10|2016|      5|\n",
      "|2016-03-18| 18|  11|    3|2016|      6|\n",
      "|2016-06-15| 15|  24|    6|2016|      4|\n",
      "+----------+---+----+-----+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table = spark.read.parquet(\"sas_data2/time\")                 \n",
    "time_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|airline|fltno|\n",
      "+-------+-----+\n",
      "|     AF|00077|\n",
      "|     BA|00271|\n",
      "|     UA|00008|\n",
      "|     AA|01489|\n",
      "|     OB|00766|\n",
      "|     DL|00469|\n",
      "|     UA|00037|\n",
      "|     NH|00172|\n",
      "|     2D|00412|\n",
      "|     KE|   62|\n",
      "|     DL|00749|\n",
      "|     AC|00757|\n",
      "|     AV|00042|\n",
      "|     WS|01118|\n",
      "|     UA|05547|\n",
      "|     UA|01691|\n",
      "|     UA|03928|\n",
      "|     AA|01739|\n",
      "|     LH|  464|\n",
      "|     AA| 1526|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights = spark.read.parquet(\"sas_data2/flights\")\n",
    "flights.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|i94visa|visatype|\n",
      "+-------+--------+\n",
      "|      2|      CP|\n",
      "|      2|     SBP|\n",
      "|      1|     GMB|\n",
      "|      2|     CPL|\n",
      "|      2|     GMT|\n",
      "|      3|      F2|\n",
      "|      1|      E2|\n",
      "|      3|      M1|\n",
      "|      1|      B1|\n",
      "|      1|      I1|\n",
      "|      1|      WB|\n",
      "|      2|      B2|\n",
      "|      3|      F1|\n",
      "|      1|      E1|\n",
      "|      2|      WT|\n",
      "|      3|      M2|\n",
      "|      1|       I|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visa = spark.read.parquet(\"sas_data2/visas\")\n",
    "visa.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----+-----+----+-------+\n",
      "|   arrdate|day|week|month|year|weekday|\n",
      "+----------+---+----+-----+----+-------+\n",
      "|2016-04-10| 10|  14|    4|2016|      1|\n",
      "|2016-04-24| 24|  16|    4|2016|      1|\n",
      "|2016-04-11| 11|  15|    4|2016|      2|\n",
      "|2016-04-25| 25|  17|    4|2016|      2|\n",
      "|2016-04-15| 15|  15|    4|2016|      6|\n",
      "|2016-04-21| 21|  16|    4|2016|      5|\n",
      "|2016-04-03|  3|  13|    4|2016|      1|\n",
      "|2016-04-04|  4|  14|    4|2016|      2|\n",
      "|2016-04-14| 14|  15|    4|2016|      5|\n",
      "|2016-04-26| 26|  17|    4|2016|      3|\n",
      "|2016-04-13| 13|  15|    4|2016|      4|\n",
      "|2016-04-09|  9|  14|    4|2016|      7|\n",
      "|2016-04-02|  2|  13|    4|2016|      7|\n",
      "|2016-04-16| 16|  15|    4|2016|      7|\n",
      "|2016-04-06|  6|  14|    4|2016|      4|\n",
      "|2016-04-08|  8|  14|    4|2016|      6|\n",
      "|2016-04-23| 23|  16|    4|2016|      7|\n",
      "|2016-04-30| 30|  17|    4|2016|      7|\n",
      "|2016-04-01|  1|  13|    4|2016|      6|\n",
      "|2016-04-29| 29|  17|    4|2016|      6|\n",
      "+----------+---+----+-----+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time = spark.read.parquet(\"sas_data2/time\")\n",
    "time.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+------+-------+-------+-------+-----+--------+----+---+\n",
      "|cicid|   arrdate|i94yr|i94mon|i94visa|i94port|airline|fltno|visatype|  yr|mon|\n",
      "+-----+----------+-----+------+-------+-------+-------+-----+--------+----+---+\n",
      "|  225|2016-04-01| 2016|     4|      2|    NYC|     OS|00087|      WT|2016|  4|\n",
      "|  598|2016-04-01| 2016|     4|      2|    NAS|     UP|00221|      WT|2016|  4|\n",
      "|  604|2016-04-01| 2016|     4|      1|    TOR|     AA|01259|      WB|2016|  4|\n",
      "| 1053|2016-04-01| 2016|     4|      2|    NEW|     DL|00021|      WT|2016|  4|\n",
      "| 1164|2016-04-01| 2016|     4|      2|    WAS|     UA|00947|      WT|2016|  4|\n",
      "| 1391|2016-04-01| 2016|     4|      2|    NYC|     SN|01401|      WT|2016|  4|\n",
      "| 1405|2016-04-01| 2016|     4|      2|    NYC|     AF|00012|      WT|2016|  4|\n",
      "| 1463|2016-04-01| 2016|     4|      2|    NYC|     DL|00049|      WT|2016|  4|\n",
      "| 1695|2016-04-01| 2016|     4|      2|    MIA|     UX|00097|      WT|2016|  4|\n",
      "| 2222|2016-04-01| 2016|     4|      2|    ORL|     LH|00464|      B2|2016|  4|\n",
      "| 2226|2016-04-01| 2016|     4|      2|    NYC|     DY|07015|      B2|2016|  4|\n",
      "| 2369|2016-04-01| 2016|     4|      2|    NYC|     LO|00026|      B2|2016|  4|\n",
      "| 2612|2016-04-01| 2016|     4|      2|    NYC|     LO|00026|      B2|2016|  4|\n",
      "| 2990|2016-04-01| 2016|     4|      2|    NEW|     SK|00909|      WT|2016|  4|\n",
      "| 3028|2016-04-01| 2016|     4|      2|    NEW|     FI|00623|      WT|2016|  4|\n",
      "| 3091|2016-04-01| 2016|     4|      2|    NYC|     DY|07011|      WT|2016|  4|\n",
      "| 3553|2016-04-01| 2016|     4|      2|    SAJ|     DY|07125|      WT|2016|  4|\n",
      "| 3690|2016-04-01| 2016|     4|      2|    NYC|     KL|00641|      WT|2016|  4|\n",
      "| 4276|2016-04-01| 2016|     4|      2|    SFR|     LH|00454|      WT|2016|  4|\n",
      "| 4512|2016-04-01| 2016|     4|      1|    ATL|     DL|00085|      WB|2016|  4|\n",
      "+-----+----------+-----+------+-------+-------+-------+-----+--------+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_imm = spark.read.parquet(\"sas_data2/immigration\")\n",
    "df_imm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
